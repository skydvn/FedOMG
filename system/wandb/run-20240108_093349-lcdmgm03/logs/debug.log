2024-01-08 09:33:49,085 INFO    MainThread:24663 [wandb_setup.py:_flush():76] Current SDK version is 0.16.0
2024-01-08 09:33:49,085 INFO    MainThread:24663 [wandb_setup.py:_flush():76] Configure stats pid to 24663
2024-01-08 09:33:49,085 INFO    MainThread:24663 [wandb_setup.py:_flush():76] Loading settings from /home/duong/.config/wandb/settings
2024-01-08 09:33:49,085 INFO    MainThread:24663 [wandb_setup.py:_flush():76] Loading settings from /home/duong/Documents/git/FederatedLearning/pFL-CAG/system/wandb/settings
2024-01-08 09:33:49,085 INFO    MainThread:24663 [wandb_setup.py:_flush():76] Loading settings from environment variables: {}
2024-01-08 09:33:49,085 INFO    MainThread:24663 [wandb_setup.py:_flush():76] Applying setup settings: {'_disable_service': False}
2024-01-08 09:33:49,085 INFO    MainThread:24663 [wandb_setup.py:_flush():76] Inferring run settings from compute environment: {'program_relpath': 'system/main.py', 'program_abspath': '/home/duong/Documents/git/FederatedLearning/pFL-CAG/system/main.py', 'program': '/home/duong/Documents/git/FederatedLearning/pFL-CAG/system/main.py'}
2024-01-08 09:33:49,085 INFO    MainThread:24663 [wandb_setup.py:_flush():76] Applying login settings: {'force': True}
2024-01-08 09:33:49,085 INFO    MainThread:24663 [wandb_init.py:_log_setup():524] Logging user logs to /home/duong/Documents/git/FederatedLearning/pFL-CAG/system/wandb/run-20240108_093349-lcdmgm03/logs/debug.log
2024-01-08 09:33:49,085 INFO    MainThread:24663 [wandb_init.py:_log_setup():525] Logging internal logs to /home/duong/Documents/git/FederatedLearning/pFL-CAG/system/wandb/run-20240108_093349-lcdmgm03/logs/debug-internal.log
2024-01-08 09:33:49,085 INFO    MainThread:24663 [wandb_init.py:init():564] calling init triggers
2024-01-08 09:33:49,085 INFO    MainThread:24663 [wandb_init.py:init():571] wandb.init called with sweep_config: {}
config: {'goal': 'test', 'device': 'cuda', 'device_id': '0', 'dataset': 'cifar10', 'num_classes': 10, 'model_str': 'resnet8', 'model': ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer_0): BasicBlock(
    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (layer_1): BasicBlock(
    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (downsample): Sequential(
      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer_2): BasicBlock(
    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (downsample): Sequential(
      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): Sequential(
    (0): AdaptiveAvgPool2d(output_size=(1, 1))
    (1): Flatten(start_dim=1, end_dim=-1)
  )
  (fc): Linear(in_features=256, out_features=10, bias=True)
), 'batch_size': 32, 'local_learning_rate': 0.005, 'learning_rate_decay': False, 'learning_rate_decay_gamma': 0.99, 'global_rounds': 20, 'local_epochs': 5, 'algorithm': 'FedTest', 'join_ratio': 1.0, 'random_join_ratio': False, 'num_clients': 20, 'prev': 0, 'times': 1, 'eval_gap': 1, 'privacy': False, 'dp_sigma': 0.0, 'save_folder_name': 'items', 'auto_break': False, 'dlg_eval': False, 'dlg_gap': 100, 'batch_num_per_client': 2, 'num_new_clients': 0, 'fine_tuning_epoch': 0, 'log': True, 'noniid': True, 'balance': True, 'alpha_dirich': 0.1, 'client_drop_rate': 0.0, 'train_slow_rate': 0.0, 'send_slow_rate': 0.0, 'time_select': False, 'time_threthold': 10000, 'beta': 0.0, 'lamda': 1.0, 'mu': 0, 'K': 5, 'p_learning_rate': 0.01, 'M': 5, 'itk': 4000, 'alphaK': 1.0, 'sigma': 1.0, 'alpha': 1.0, 'plocal_steps': 1, 'tau': 1.0, 'fine_tuning_steps': 10, 'dr_learning_rate': 0.0, 'L': 1.0, 'noise_dim': 512, 'generator_learning_rate': 0.005, 'hidden_dim': 512, 'server_epochs': 1000, 'localize_feature_extractor': False, 'server_learning_rate': 1.0, 'eta': 1.0, 'rand_percent': 80, 'layer_idx': 2, 'mentee_learning_rate': 0.005, 'T_start': 0.95, 'T_end': 0.98, 'lamda_reg': 0.0, 'cagrad_rounds': 100, 'cagrad_learning_rate': 25.0, 'momentum': 0.5, 'step_size': 30, 'gamma': 0.5, 'run_name': 'FedTest__cifar10__20__1704681228'}
2024-01-08 09:33:49,085 INFO    MainThread:24663 [wandb_init.py:init():614] starting backend
2024-01-08 09:33:49,085 INFO    MainThread:24663 [wandb_init.py:init():618] setting up manager
2024-01-08 09:33:49,086 INFO    MainThread:24663 [backend.py:_multiprocessing_setup():105] multiprocessing start_methods=fork,spawn,forkserver, using: spawn
2024-01-08 09:33:49,087 INFO    MainThread:24663 [wandb_init.py:init():624] backend started and connected
2024-01-08 09:33:49,089 INFO    MainThread:24663 [wandb_init.py:init():716] updated telemetry
2024-01-08 09:33:49,091 INFO    MainThread:24663 [wandb_init.py:init():749] communicating run to backend with 90.0 second timeout
2024-01-08 09:33:49,716 INFO    MainThread:24663 [wandb_run.py:_on_init():2254] communicating current version
2024-01-08 09:33:49,956 INFO    MainThread:24663 [wandb_run.py:_on_init():2263] got version response upgrade_message: "wandb version 0.16.1 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"

2024-01-08 09:33:49,956 INFO    MainThread:24663 [wandb_init.py:init():800] starting run threads in backend
2024-01-08 09:33:49,982 INFO    MainThread:24663 [wandb_run.py:_console_start():2233] atexit reg
2024-01-08 09:33:49,982 INFO    MainThread:24663 [wandb_run.py:_redirect():2088] redirect: wrap_raw
2024-01-08 09:33:49,982 INFO    MainThread:24663 [wandb_run.py:_redirect():2153] Wrapping output streams.
2024-01-08 09:33:49,982 INFO    MainThread:24663 [wandb_run.py:_redirect():2178] Redirects installed.
2024-01-08 09:33:49,982 INFO    MainThread:24663 [wandb_init.py:init():841] run started, returning control to user process
2024-01-08 09:37:30,435 WARNING MsgRouterThr:24663 [router.py:message_loop():77] message_loop has been closed
